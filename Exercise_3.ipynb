{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DO211IMKk-Kg"
   },
   "source": [
    "# **Plant Disease Classification Using CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XayUS_Nky0e"
   },
   "source": [
    "**About Dataset**\n",
    "\n",
    "We have used **plant_village** dataset for classification. The dataset consist of 4234 images of apple leaves. The dataset is divided into four classes namely Apple_scab, Frogeye_Spot, Cedar_apple_rust, and Healthy.\n",
    "\n",
    "* Input image size: 64 * 64 * 3\n",
    "\n",
    "* In the dataset :\n",
    "    Training Set - 70%,\n",
    "    Validation Set - 20%,\n",
    "    Test Set - 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyB6J9ZB1pCa"
   },
   "source": [
    "#**Importing Necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JuKSi3EJyGb-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 08:01:16.616442: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dHGxV5lHVuWG",
    "outputId": "3f17be7d-969d-44b9-8b67-dc266ecd37e2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHnd3rrLLql4"
   },
   "source": [
    "#**Loading data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PyYRaHdLavO2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# i = 0\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(data_path):\n\u001b[1;32m      8\u001b[0m     img_list\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mlistdir(data_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m dataset)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded the images of dataset-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "data_path= 'path to dataset'\n",
    "\n",
    "img_data_list=[]\n",
    "labels = []\n",
    "\n",
    "# i = 0\n",
    "for dataset in os.listdir(data_path):\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    # i = 0\n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "        input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "        labels.append(dataset)\n",
    "        # print(input_img.shape)\n",
    "        input_img_resize=cv2.resize(input_img,(224,224))\n",
    "        input_img_resize=cv2.normalize(input_img_resize,None,0,255,cv2.NORM_MINMAX)\n",
    "        img_data_list.append(input_img_resize)\n",
    "\n",
    "labels=np.array(labels)\n",
    "img_data = np.array(img_data_list)\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwNwRorH8g2W"
   },
   "source": [
    "# **Visualization of few images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZsBwS0oavRB"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(1, 5):\n",
    "  plt.imshow(img_data[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AheZPiNC6ZwZ"
   },
   "outputs": [],
   "source": [
    "# Convert label to one-hot encoded matrix\n",
    "from keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6SwOHgNQqWR"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(img_data, label, test_size=0.1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2)\n",
    "\n",
    "# Print the train set\n",
    "print('Train Set')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "# Print the validation set\n",
    "print('Validation Set')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Print the test set\n",
    "print('Test Set')\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiJvRIxN5P9D"
   },
   "source": [
    "\n",
    "# **Model Definition**\n",
    "\n",
    "* We are going to use 8 convolution layers with 3*3 filer and relu as an activation function\n",
    "* Then max pooling layer with 2*2 filter is used\n",
    "* After that we are going to use Flatten layer\n",
    "* Then Dense layer is used with relu function\n",
    "* In the output layer softmax function is used with 4 neurons as we have four class dataset.\n",
    "* model.summary() is used to check the overall architecture of the model with number of learnable parameters\n",
    "* Padding is Valid (no padding).\n",
    "* Stride is 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE56efIVOG5I"
   },
   "source": [
    "model.summary() gives information about output shape and number of parameters in each layer.\n",
    "\n",
    "**Output shape** can be calculated using following equation:\n",
    "\n",
    "$$ shape_w = \\frac{w-f+2p}{s}+1$$\n",
    "\n",
    "$$ shape_h = \\frac{h-f+2p}{s}+1$$\n",
    "\n",
    "**Parameter size** can be obtained using below equation:\n",
    "\n",
    "$$ parameter = (filter_{size} * N^{channel}_{previous}*N^{filter}_{current}) + (N^{bias} * N^{filter}_{current}) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTEBdXgBmBn-"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = models.Sequential()\n",
    "# Add new layers\n",
    "model.add(Conv2D(16, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu', input_shape=(224,224,3)))\n",
    "model.add(Conv2D(32, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(Conv2D(128, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense('write number of classes', activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EuUhICY2dGq"
   },
   "source": [
    "#**Compiling and Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es_cvLHSmIbF"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = optimizers.Adam(learning_rate = 0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit('enter training data', epochs=80, batch_size=16, validation_data=('enter validation data'), verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcVJwaJOmL-i"
   },
   "source": [
    "# **Saving the model**\n",
    "\n",
    "An H5 file is a data file saved in the Hierarchical Data Format (HDF). HDF5 lets you store huge amounts of numerical data, and easily manipulate that data from NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULc2T2mgKTLA"
   },
   "outputs": [],
   "source": [
    "model.save(\"path to save your model\")\n",
    "print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DoJsC_0mT01"
   },
   "source": [
    "# **Loading the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMSvq1fjmVwQ"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('path of your model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpJ96F4b2jCD"
   },
   "source": [
    "#**Visualization of Accuracy and Loss Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StfAuMLuKfCi"
   },
   "outputs": [],
   "source": [
    "train_acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKGbd7lvKfEY"
   },
   "outputs": [],
   "source": [
    "epochs = range(len(train_acc))\n",
    "plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'g', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs, train_loss, 'b', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'g', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWu3BUkb2sJY"
   },
   "source": [
    "#**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9UL05zbKfGR"
   },
   "outputs": [],
   "source": [
    "# Get the ground truth\n",
    "ground_truth = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Get the predictions from the model\n",
    "predictions = model.predict(x_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "errors = np.where(predicted_classes != ground_truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors), y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxXivh_jKfIg"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(ground_truth, predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj4x6pbq2vdn"
   },
   "source": [
    "#**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1-Cw2BiKfL0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "cm = confusion_matrix(y_true=ground_truth, y_pred=predicted_classes)\n",
    "cm = np.array(cm)\n",
    "# Normalise\n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=label2index, yticklabels=label2index, cmap=\"YlGnBu\")\n",
    "plt.ylabel('Actual', fontsize=15)\n",
    "plt.xlabel('Predicted', fontsize=15)\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxArS40N2y6M"
   },
   "source": [
    "#**Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-jcVMDfKTND"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ground_truth, predicted_classes, target_names=label2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrVPX62BVbnw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxV3h4hcUByx"
   },
   "source": [
    "#**Finetuning Hyper-parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5DU3US2a2O5"
   },
   "outputs": [],
   "source": [
    "def create_model(learn_rate=0.01, momentum=0,opt):\n",
    "    image_size = 224\n",
    "    input_shape = (image_size, image_size, 3)\n",
    "\n",
    "    # Create the model\n",
    "    model = models.Sequential()\n",
    "    # Add new layers\n",
    "    model.add(Conv2D(16, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu', input_shape=(64,64,3)))\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "    model.add(Conv2D(256, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "    model.add(Conv2D(256, kernel_size=(3,3), strides = 1, padding = 'valid', activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(4, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # model = Model(input_shape, x)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.SGD(lr=learn_rate, momentum=momentum),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMbbjnUJYd4E"
   },
   "outputs": [],
   "source": [
    "learn_rate = [1e-9, 1e-3]\n",
    "momentum = [0.6, 0.9]\n",
    "optimizer = [sgd,]\n",
    "\n",
    "\n",
    "def try_fit(learn_rate,momentum):\n",
    "    history_page=[]\n",
    "    for lr in learn_rate:\n",
    "        for moment in momentum:\n",
    "          for opt i optmizer:\n",
    "            model = create_model(lr,moment,opt)\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                epochs=1,\n",
    "                validation_data=validation_generator)\n",
    "            history_page.append(history)\n",
    "    return history_page\n",
    "\n",
    "history_page = try_fit(learn_rate,momentum)\n",
    "history_page[0].history['accuracy']"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
