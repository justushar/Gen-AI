{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CZzWMYQGcUuj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "o-AJ_7BHco6p"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04zflwiRcrzw",
        "outputId": "abce7764-7dd3-4bdf-edc6-eb7b7f8e2a4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAwUNRMpc6rl",
        "outputId": "055983e6-18a1-4997-e66f-7f6314807a8c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6oOKZFKcxTt",
        "outputId": "4a49ef12-b97b-43b7-d7c7-996dc35d10c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5Iu2uO8eVg6",
        "outputId": "934944c4-b433-4e3c-9e2e-1ae086e277c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "0Tj3GXYSeXPx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSoSF1ayeZNR",
        "outputId": "047c120c-e832-4833-9c81-194d94d8e844"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "hPY9qjGGea2C"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oROAE4FfecbK",
        "outputId": "22fd4e84-62bc-436a-9c38-75d0a71a0977"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWZE35ydeeP6",
        "outputId": "100f0ce7-ee6f-40dd-bca7-de1534c5921d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "4zZ3tZtHegKY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As74QiVaekUi",
        "outputId": "3e2406c1-dbdb-4b29-936e-299f2f7f0db3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydKAZhEPemhG",
        "outputId": "6f0a562d-f693-4db1-9f7b-9794cf2105ab"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxHZNojpeqvj",
        "outputId": "effbd92c-c7c0-4643-c84e-4bc97b0926e8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5AS0UFNevRx",
        "outputId": "7ddcb2e8-a76e-41f3-eaa1-dfe66eee71d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "sjJcq76Iexi7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH8PahnOgLQp",
        "outputId": "29eb14da-b7fe-4983-ab87-06f393f7fe34"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEUENd-ie2JG",
        "outputId": "a93cc748-4893-42a7-a187-61c4fb8be94e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua0lXUiOe61m",
        "outputId": "a208673a-029a-457d-ad4a-7ad160a78f1e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "2kHhZBk5e8ro"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "tkye_iQOe-sR"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "9Lg5qS7Be__g"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJa4Or-8fBbC",
        "outputId": "d6deb3a6-84df-42c7-c1df-238cfd8a6f35"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thcx_LaBfC9f",
        "outputId": "e02008db-4261-4ee7-8e86-ee3f6d6c6f6a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "RTMRJhYsfEqB"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Y0dFbFfGMg",
        "outputId": "1065a567-78b9-4797-e3b5-85beee0d8985"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([39, 15, 24, 37, 32, 64, 10, 54,  5, 29, 44, 55, 61, 43, 53, 58, 47,\n",
              "        6,  3, 52, 41,  3, 27, 14, 65,  3, 62, 57,  0, 42, 13,  1, 47, 12,\n",
              "       64, 17, 23, 50, 52, 22, 41, 16, 27, 44, 29, 30, 57,  6, 48, 43, 31,\n",
              "       21, 48, 25,  8,  4, 23, 45, 45, 51, 23, 35,  7, 64, 17, 56, 28,  4,\n",
              "       34, 44, 37, 58, 29, 65, 62, 36, 18, 62, 53, 41, 34,  8, 15, 65, 51,\n",
              "       19, 30,  8, 36, 52, 22, 32, 34, 14, 40, 15, 30, 11, 13,  3])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxIHeO-VfHvi",
        "outputId": "1016a240-8dbb-46c4-ccae-9b2f9fa60268"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"ings you'll say a beggar nay.\\n\\nGLOUCESTER:\\nIt is too heavy for your grace to wear.\\n\\nYORK:\\nI weigh it\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"ZBKXSy3o&Pepvdnsh'!mb!NAz!wr[UNK]c?\\nh;yDJkmIbCNePQr'idRHiL-$JfflJV,yDqO$UeXsPzwWEwnbU-BzlFQ-WmISUAaBQ:?!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7tDAOwffJN0",
        "outputId": "a4a73223-045e-4d97-9e69-766f8969489d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1904664, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCSp28d6fK3Z",
        "outputId": "f6204fb2-0672-4198-ba76-752d143b96f8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.05359"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "G0m_OkAYfOsm"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "6k_z0d_OfYPB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BstxYy7AfYq9",
        "outputId": "26268d3e-e0c9-4801-d3c8-6c05abe6afb5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 13s 53ms/step - loss: 2.7260\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.9981\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.7260\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.5637\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.4630\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.3938\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.3418\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.2976\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2566\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.2195\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1810\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1418\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.1011\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0570\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.0112\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9622\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9119\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8592\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.8073\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "GI85o1xpfdCs"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "iNSwnUgpffad"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwD6hRCofhGs",
        "outputId": "1a7d75a5-75bb-44c6-c1a7-8d7bc979d731"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The queen is comfort; which are the\n",
            "dire to-morrow are you by confession; yet,\n",
            "Even for the father of a wife:\n",
            "Now my good friend proclamation, here rid death\n",
            "With head and trumpets, are towards Counsel:\n",
            "My treschange enjoy, thou stendge whips?\n",
            "Here's Lucentio.\n",
            "\n",
            "LUCIO:\n",
            "This is the chair,\n",
            "She is my mouths, he will shake your tears:\n",
            "Who now is he was lest his son so weary\n",
            "That rooted to find Juliet castle's warge\n",
            "Till the wisdom of the eagle's head\n",
            "And has The queen's soon-heady-hoodings. Upon\n",
            "Lict reverence, which else departs make trush.\n",
            "\n",
            "VOLUMNIA:\n",
            "No, by an under-take much.\n",
            "York, good morrow, Ratcliff!-\n",
            "MARCIUS:\n",
            "A boldness of the Earl of Mercutio?\n",
            "\n",
            "TRANIO:\n",
            "Master, master! how far inity may never growind\n",
            "To be o'er eye's right from him; or she, perchavio,\n",
            "And these eyes stempt provosting of;\n",
            "But I am sure of; upon your honour.\n",
            "\n",
            "Nurse:\n",
            "Her oath, I hear her reciond from his blood,\n",
            "And from my life closed by the quarrel of the fox\n",
            "And shows a man of chief; a bawd of revenge.\n",
            "\n",
            "CLIFFORD:\n",
            "Th \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.1039907932281494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV3w_A8_fiUm",
        "outputId": "9668fea6-d70e-4bc0-ba85-b219b10de100"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThe gates made good, my lord; that it comes home\\nThat valiant in the walls, and try sword, one more\\nWisdom of the envious daughters: here thou art\\nAn hour shef blood i' the world's; and thou hast heard\\nsneep doth cavorrow makes me wed.\\n\\nBALONAS:\\nAnd Master Calian: 'tis no last to hinder of him.\\n\\nESCALUS:\\nMercutio!\\n\\nLUCIO:\\nGood my friends; good monours, nothing;\\nOne that Baptista, cried to Romeo,\\nAlike and thus royal age.\\n\\nRICHARD:\\nRomeo is ease to wife; who hath got in's ears.\\n\\nSecond Servingman:\\n\\nPage:\\nThis shall shake off guilty of good sister\\nTo execute the tastly through all the\\nhabit, served with bestraight:\\nGive me my bond Mortaged, did men blood cut\\nof delay, yet marry her noble across;\\nAlas, alas! shalt thou know my firm\\nI should do three look for pactifugh.\\n\\nMERCUTIO:\\n\\nANGELO:\\nYet here that is not known to be his soldiers;\\nAnd I had on the violet in the fair of it.\\n\\nWESTMORELAND:\\nChose, how let me never fetch out of you.\\nLet them do give awhile wepresh with't;\\nOr of my Richar\"\n",
            " b\"ROMEO:\\nYet may believe this great rose.\\nThe Kathalitation of a discourse, if you\\nWhilst I will acquaint her beauty, one plain.\\nSou near me be fourteen of the glory, lord,\\nShed for detto much upon our seats and watch\\nTo whom when thou hast herniuls,\\nBecause I am abroad, 'tis a lest;\\nFor I have look'd voices of the beastlish'd haste:\\nHis fault unjust's rans extermined,\\nAnd all the queen your daughter Katharina,\\nShart hath not so quickly now that bears the like notice,\\nThan seek thee in the tick bay when the man\\nI admit down.\\n\\nKING RICHARD II:\\nWe will be: your heart, are you at hands.\\n\\nFirst Citizen:\\nYou will warm myself a hang: remember more;\\nAnd therefore, ere I woo'd in such gracious enemies;\\nFor never crowned him, and I hasp there too.\\nThou wilt any jest, if I must answer her.\\n\\nLEONTES:\\nRepent is too cold; always, and ears\\nCall all rest becomes restories; where action,\\nNever-dangerous, as the matters of his lookships\\nAs for my other walls,\\nReceives it to a brace of honour.\\n\\nNORTHUMBERLAND:\\n\"\n",
            " b\"ROMEO:\\nThe father is back to us: it in extremity,\\nWhich to maintain you. Harry, you need of not\\nCorn for the house of my misicice.\\n\\nANGELO:\\nYet show them merry when I am a lower:\\nTell him, my heart it explemes, that colours\\nEnforced the general carefal blerdin\\nA thing detrected shortly, throwers to his oath,\\nTeach her that they are never in my head\\nAnd womb'd out for your dayling: yet, thou'rt patience;\\nCome, I dispatch, 'twere pity me: we say too.\\n\\nMENENIUS:\\nIf it were faces bad is more than beauty,\\nThat taught his knee before a hope, the king is fall:\\nNow that the strong soul dit on the god, thy uncle, free\\nAs private for ever: Sicinius good night,\\nTo make was her than the king; and himself come.\\n\\nHORTENSIO:\\nNow, Signior Gremio.\\n\\nTRANIO:\\nGentlemen, between us, it shall be so violent\\nAs boundly, or one oratle: may my hand will\\nnot receive. I am no better, beside\\nYou do not mean smull upon that looks!\\nNow Romeo be brieved fearful father.\\nLook for this; by that dance extremest should\\nbe sus i\"\n",
            " b\"ROMEO:\\nThen give me leave, to quench these rags,\\nReplined, my waying and of me,\\nThat seem'd if we beholding to the feast, and\\nthere not miliner. But, sir, in death\\nThat bear the live that fally upon my cousin's skin,\\nAnd good my nutarcest shrew.\\n\\nThird Citizen:\\nHe's a little done, and attend your hatred thee,\\nBut despised to my love and heavy and minate:\\nSirrahable is extempore.' Butch, who is born to Salisbury\\nAnd was an executed in their entire;\\nI'll tell my tribe: that I desire to bring them,\\nThen few of spirit! most degrees are too much,\\nYet ever, exercise, how far off it?\\n\\nBENVOLIO:\\nRofe, sir! come on, but once a garden thy fortune,\\nAnd I must not kneel those grace within awn.\\nBut, more than I, forward, forward, unless:\\nYou do but love that know how to teach when thou hast\\nslaughter me.\\n\\nSLAMILLO:\\nBachasin, sir, am break at, provost.\\n\\nPAULINA:\\nThese griefs, more question.\\n\\nROMEO:\\nLet them know my touches;\\nYet aver, sir, if you call him so question\\nIs the dot nature with her gentlemen th\"\n",
            " b\"ROMEO:\\nThe good side, green ballad, a nobleman,\\nHath wars death to live or do?\\nOr ary of it? both with the land of other blossoms\\nAs drink for yours' large dreaming where I ask.\\n\\nKING RICHARD II:\\nAs I have said, my lord, to thee ago, to\\ngranter that no man here brone the hours of mine,\\nWhich, by the next recorp, Off with the place,\\nAnd I will endew chance again to\\nBonn and Somerset and her riche-by\\nThrow thy peruling title of his grow holford.\\n\\nKING HENRY VI:\\nPart thou not now my lord thou shalt not do;\\nObsoldiness a rarches newly show my dump'd,\\nThat all the world's dovenes of death,\\ncall you and yet: the gods on the poor form\\nHaving the love I bear your loves,\\nFrom off what thou hast too much:\\nThe curring rage of it, may ever trust\\nPiordel'd has, and with him for my misery,\\nFor I have more cause to Brison; we\\ncan forswear you to have a voise of Rome.\\n\\nSICINIUS:\\nI would thou hadst not pladed with brow!\\nDost complain on the wars be murder, thank you.\\n\\nGREMIO:\\nTell homes upon a scrange one. I\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.672757387161255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nqiMnSdfkLw",
        "outputId": "63103147-cfec-46df-f497-3999deddd03f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7adc5066bdf0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYS7gDuWfl59",
        "outputId": "7fedf5f8-81fd-4729-b1dc-756b7687fcbc"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The other for a manfor, stand upon!\n",
            "Thou straight, my lord, to give them in;\n",
            "and I will steal, alon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "hYCQGgM_fuV1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "o5ePY7Cjfuxj"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "9N_cOvIUfwjS"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsoo7UHnfyhp",
        "outputId": "f7cc9d76-99f2-462d-cba9-64a0a6abe357"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 58ms/step - loss: 2.7208\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7adc4814bcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fgp-eX8f0S_",
        "outputId": "e18fc14c-07db-4602-9281-dd904f4159ec"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1839\n",
            "Epoch 1 Batch 50 Loss 2.0628\n",
            "Epoch 1 Batch 100 Loss 1.9504\n",
            "Epoch 1 Batch 150 Loss 1.8178\n",
            "\n",
            "Epoch 1 Loss: 1.9849\n",
            "Time taken for 1 epoch 12.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8117\n",
            "Epoch 2 Batch 50 Loss 1.7447\n",
            "Epoch 2 Batch 100 Loss 1.6887\n",
            "Epoch 2 Batch 150 Loss 1.6905\n",
            "\n",
            "Epoch 2 Loss: 1.7064\n",
            "Time taken for 1 epoch 11.50 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5821\n",
            "Epoch 3 Batch 50 Loss 1.5966\n",
            "Epoch 3 Batch 100 Loss 1.5162\n",
            "Epoch 3 Batch 150 Loss 1.5285\n",
            "\n",
            "Epoch 3 Loss: 1.5466\n",
            "Time taken for 1 epoch 11.29 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4563\n",
            "Epoch 4 Batch 50 Loss 1.4625\n",
            "Epoch 4 Batch 100 Loss 1.4883\n",
            "Epoch 4 Batch 150 Loss 1.4610\n",
            "\n",
            "Epoch 4 Loss: 1.4483\n",
            "Time taken for 1 epoch 10.94 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4166\n",
            "Epoch 5 Batch 50 Loss 1.3965\n",
            "Epoch 5 Batch 100 Loss 1.3825\n",
            "Epoch 5 Batch 150 Loss 1.3647\n",
            "\n",
            "Epoch 5 Loss: 1.3803\n",
            "Time taken for 1 epoch 11.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3464\n",
            "Epoch 6 Batch 50 Loss 1.3110\n",
            "Epoch 6 Batch 100 Loss 1.3313\n",
            "Epoch 6 Batch 150 Loss 1.3454\n",
            "\n",
            "Epoch 6 Loss: 1.3277\n",
            "Time taken for 1 epoch 10.92 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2785\n",
            "Epoch 7 Batch 50 Loss 1.2479\n",
            "Epoch 7 Batch 100 Loss 1.2830\n",
            "Epoch 7 Batch 150 Loss 1.3146\n",
            "\n",
            "Epoch 7 Loss: 1.2826\n",
            "Time taken for 1 epoch 11.03 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2172\n",
            "Epoch 8 Batch 50 Loss 1.2247\n",
            "Epoch 8 Batch 100 Loss 1.2589\n",
            "Epoch 8 Batch 150 Loss 1.2450\n",
            "\n",
            "Epoch 8 Loss: 1.2411\n",
            "Time taken for 1 epoch 11.14 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1698\n",
            "Epoch 9 Batch 50 Loss 1.1502\n",
            "Epoch 9 Batch 100 Loss 1.2025\n",
            "Epoch 9 Batch 150 Loss 1.1971\n",
            "\n",
            "Epoch 9 Loss: 1.2010\n",
            "Time taken for 1 epoch 11.03 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1483\n",
            "Epoch 10 Batch 50 Loss 1.1252\n",
            "Epoch 10 Batch 100 Loss 1.2076\n",
            "Epoch 10 Batch 150 Loss 1.1781\n",
            "\n",
            "Epoch 10 Loss: 1.1607\n",
            "Time taken for 1 epoch 11.15 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4psWWFwf2hB"
      },
      "execution_count": 62,
      "outputs": []
    }
  ]
}